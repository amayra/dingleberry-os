This is a small hobby kernel. Is it useful for anything? Not at all. Purpose is
a combination of ergotherapy and self-harm.

But what does it do? It prints "Hello world." to the qemu serial console. In
case I keep developing on this, the way it prints this message will become
increasingly more complex (but still "Hello world." on a serial console).

Blog
====

In the following sections I'll blog about random crap because nobody cares.

What is hobby OS development
============================

Pointless.

Ah, let's try again...

Hobby OS development deals with the exploration of OS concepts, in particular
the kernel. People start by implementing the kernel, and if they get as far,
maybe a bunch of drivers and userspace parts. You can focus on how to implement
kernel mechanisms (threads, mutexes, etc.), maybe how to implement the
equivalents in userspace on top of the kernel. Or you can focus on operating
system concepts, for example microkernel vs. something else, or concrete stuff,
like how to design the filesystem interface. Or you can focus on accessing
various hardware and implementing drivers.

Note that in academic context it's called "OS research". The main difference is
that they're paid for it.

There are also parts of OSDEV you can skip over, like implementing a libc.
Most people will probably not implement a compiler or language, and just use
gcc and C (or if you have bad taste, clang and C++).

This repository probably focuses on OS concepts, or just wasting time.

It's never finished and it's pointless
--------------------------------------

An OS is a very large project. As a single person, it'll never be finished,
unless you're Linus or a mentally ill person (see TempleOS). The main point of
it is learning, and smugly demonstrating to others that you're able to master
the low level things.

So this is a learning experience (some use the pejorative "education"), not to
produce something useful. What it _could_ produce by exploring concepts are good
ideas (some use the pejorative "research").

Kernel
------

Most people will be sane and use the traditional kernel space and userspace
separation. You could be crazy and make a single address space OS. For the
latter, there is TempleOS for mentally ill grade, or something like Java OSes
or Microsoft Singularity for sane-but-dumb. (The latter mostly focus on
achieving privilege separation on the language level, so it might be
interesting if you want to create your own compiler. But a bit of a big project
to attack both compilers and OSes at once.)

For normal native code kernel/userspace OSes, you'll essentially start by
creating a primitive not-"kernel", that prints "hello world" in privileged
mode. Then you can work yourself forward by for example implementing kernel
memory management, and eventually a userspace process. At this point it'll be
obvious (but still hard) how to progress. The question will come up what kind
of system you're trying to create. The recommendation is creating a UNIX/POSIX
kernel, because that is the simplest and most proven OS design.

Libc
----

You'll probably need basic tools like printf() and memcpy(). My recommendation
is to either write them yourself (a bit of work and a stumbling block every time
you hit an unimplemented function), or to use a simple libc like musl.

musl is pretty simple, yet very complete and correct. They aim for full
C11/POSIX support. Unfortunately, the code is a bit too compact (they tend to
sacrifice compressed source code for readability), but it's better than what
you'll find in other libcs. Other libcs are either all sorts of broken and
buggy, or huge (and broken/buggy) like glibc. In fact musl also had plenty of
brokenness/bugs, but at least it's manageable due to its size. Also, musl does
not provide a bare metal mode, nor does it support OSes other than Linux. But
most likely it'll be faster to adjust musl, than porting some other libc to
your "OS" (which _will_ be a joke at such an early point anyway).

Personally I consider a libc a completely different task. If you include the
libc in OSDEV, you may as well include the compiler.

As musl shows, writing a libc yourself is possible, but a _big_ task. On the
other hand, you probably won't ever port real world software to your system,
so a partial and potentially buggy libc implementation will be perfectly fine.

Reminder to myself
------------------

I used an ad-hoc way to "redirect" musl syscalls to Linux. The musl devs are
open to better ways to retarget musl, but I don't expect much of it in the short
term. So I copied musl from the riscv-musl repository (commit 2fedc58ed98a8)
(since RISC-V isn't upstreamed yet in musl), used a musl _installation_, to
avoid some of the weird musl build script (some headers are generated by
scripts), and edited the RISC-V syscall scripts. Syscalls now call functions in
musl_emu.c. My approach breaks cancellation support (which the musl devs vividly
and repeatedly pointed out), but on the other hand cancellation is just an icky
UNIX thing that is rarely useful in the real world (due to some severe downsides
of cancellation), and which a hobby OS will never support anyway.

Build environment, linker scripts
---------------------------------

The kernel environment is similar to userspace programs, but some details are
pretty different. For example, you need to pass the correct compiler flags for
kernel development, or "bad" things could happen. This can be pretty confusing
over all. It's recommended to do what Linux does.

To get started, the most important thing is probably to interact with the
boot loader correctly. This means using a correct linker script, to make sure
the kernel is placed at the right place in memory. (I myself simply started out
with the Linux linker script for RISC-V, because linker scripts are an obscure
thing and usually full of crap.)

If your bootloader doesn't support ELF, it is recommended to use objcopy with
binary output to "flatten" it. You need to make sure that the addresses in
the ELF file matches with the addresses your sections (in fact, program headers)
are linked to.

If you want to use virtual memory, it may be necessary to relocate your kernel,
which is an extra messy part.

When you hit function calls to libgcc, look what the Linux kernel does.

What platform for hobby OS development
======================================

Obviously, an OS kernel is highly hardware dependent. A kernel may have large
parts of code that are as portable as userspace code, but even booting requires
platform-specific code. A real OS kernel like will have platform specific code
for each architecture. Of course everyone wants to be portable to everything,
but in practice you'll want to focus on one platform to get _something_ done.
Portability is hard.

It seems in OSDEV, x86 has been the preferred target architecture most of the
time,  although ARM is rising in popularity. Unfortunately, both of these
architectures are SHIT. You'll get held back by their pointless complexity and
legacy nonsense. I'm recommending RISC-V instead (see below).

x86
---

x86 is based on a tiny CPU from the 70ies. It wasn't really revolutionary or
even interesting back then, and as far as the ISA goes, it was never state of
the art in anything. It simply grew "organically", an euphemism that is used
when you only add to something without ever cleaning it up or properly designing
anything. IBM simply picked it because it was low cost or something, and then it
grew bigger along with the PC. The PC hardware isn't any better. Go search about
what the A20 gate is and why you need(ed?) to care about it, and you will
understand.

Getting started on x86 is not easy at all. You have to deal with at least 3 CPU
modes (16/32/64 bits), have to setup the weird protected mode descriptors, which
are particularly confusing because most of protected mode isn't used by modern
OSes (plus not fully implemented in 64 bit mode), and the CPU mechanisms itself
are kind of annoying (like the CPU pushing stuff on the stack when entering
exception handlers, and AFAIR _different_ stuff depending on type of exception).
Having done it myself, I really hated building CPU descriptors. Intel provides
the most performant CPUs you can buy, but as a platform, it's just not nice.

ARM
---

ARM on the other hand is a RISC architecture (i.e. followed state of the art
technology at some point) that was actually designed. Although it appears
somewhat elegant as an ISA, it approaches x86 in badness. In particular, there
are _many_ ways to build an ARM system, so you basically have to pick a specific
board. But granted, it's probably better than x86 for OSDEV. You probably can
easily find a board that supports the ISA variant you want, or qemu's virt
platform might make you happy.

RISC-V
------

RISC-V is a relatively new architecture. It's so new that it's hard to get real
hardware (not many full featured 64 bit CPUs on the market, forget about PCs).
It's basically an academic project that is now being commercialized. And maybe
due to the academic roots, the instruction set is extremely simple. While
you need to go through hundreds of pages of vendor docs to understand x86 and
ARM from basic instruction set to kernel mode operation, you can learn all of
RISC-V in an afternoon or so. Also, while the x86 and ARM docs are full of
dealing with legacy problems, the RISC-V docs are full of enlightening remarks
about ISA design and reasons for specific design choices.

RISC-V is supposed to scale from embedded to super computers, but it clearly
focuses on embedded and special-purpose applications rather than "PC" hardware,
maybe expecting even an even worse fragmented ecosystem than ARM. They expect
and _encourage_ that vendor add extensions to the ISA for special applications.
ARM does the opposite (you need a virtually unaffordable "architectural" license
to make changes to the ISA), and x86 is not available as configurable IP at all.

But they try to mitigate this by standardizing the CPU environment. There are
standard mechanisms for all the messy low level things. Some things are
abstracted by a well-defined firmware interface (that isn't a horrific
abomination like EFI, and can even be reimplemented without much trouble).

By now, toolchains and emulators are widely available and reasonably stable.
qemu provides full emulation, and gcc/binutils just work.

Thus I highly recommend RISC-V for hobby OS development. Maybe RISC-V makes it
almost too easy, and you miss out on the debugging "fun" x86 or ARM can provide.

My only gripe was with the qemu bootprocess, and the fact that you need to
provide the RISC-V firmware (OpenSBI) explicitly on the qemu CLI. Also, the
privileged mode docs are sort of unfinished (and _not_ finalized). Some things
are too vague. But it's nothing compared to what you'd have to go through on
x86.

The project in this repository uses RISC-V and requires RV64 + Sv48 MMU. This
does not exist in silicon anywhere AFAIK, so qemu with the virt platform is used
(see qemu-run.sh).

Should you make a microkernel or monolithic kernel?
===================================================

Anyone making attempts at OSdev has to deal with a million of choices they could
make. One of them is microkernel vs. monolithic kernel. Although it doesn't
matter: take this repository for example; this thing is so primitive and shitty
that the type of kernel can't even be distinguished at this stage. (I mean there
isn't even a single driver, for fuck's sake.)

Clearly the answer is that you should make a "monolithic" kernel. Just try to
make a small UNIX. Why not a microkernel? They're complex, involve tons of
design decisions (basically you need to design the entire system before deciding
on the kernel API, or it won't be very "micro"), and it will take a long time
until you get something that appears to be doing anything. Consider that real
microkernel systems don't even exist. Why fall for a hype from the 80ies that
turned out to be one of the largest failures of computer science research?
(Well, I'm not a historian, but a multiple decade effort resulting in almost
nothing seems pretty bad.)

Are Microkernels Stupid?
------------------------

Yes, they are. Both academic and commercial research made intensive efforts to
develop usable microkernels and microkernel based OSes. Absolutely nothing came
of it. Today, microkernels only have some niche uses, and OSes based on true
microkernels don't seem to exist. The height of microkernel failure was IBM's
"Workplace OS", that was a giant money grave and was eventually canceled. In
summary, microkernels didn't keep their promises.

The microkernel promise
-----------------------

Consider UNIX, which is basically the base for all other advanced OSes (WinNT
is just a UNIX buried under a lot of bloat): it has a kernel, that provides
services like hardware interactions, filesystems, process management,
networking, etc., and utilities in userspace (i.e. outside of the kernel) that
give the user access to these services. All these basic services run in the
kernel, which (from a userspace developer point of view) is a gigantic process
with a single address space. If this "process" crashes, you get a bluescreen.

For example, if someone finds a security issue in an obscure filesystem, an EVIL
HACKER could prepare a USB stick with a malicious filesystem on it, that
triggers the bug. Since there is no privilege separation between the filesystem
and the kernel, the EVIL HACKER could freely access the rest of the kernel (and
since the kernel is privileged, all of userspace), and basically do anything
with your system.

A microkernel OS would put each filesystem into a separate process. Then the bug
would, in theory, have no impact on your security at all.

Microkernels have about 2 fundamental principles: modularization, and privilege
separation of modules. They're sort of interconnected: obviously you can't do
privilege separation without some level of modularization. The "micro" here lies
in the fact that the kernel should provide the modularization and privilege
separation mechanism for "server" processes/components _only_, and all the rest
is done in userspace.

Researchers derived tons of promises from this. For example, microkernel based
OSes were claimed to be more robust and safer than monolithic OSes, because
one services are isolated, and crashes wouldn't take down the entire system (see
filesystem example above).

There were some more promises, but this isn't a damn academic paper.

Also note that microkernels did have some success in niches. But microkernel
_OSes_ did not.

Failure
-------

As mentioned above, nothing ever came of microkernels, and some of the attempts
to use them for desktop OSes ended in spectacular failures (like IBM's
Workplace OS). The main issues in general were complexity and bad performance.

I guess performance in particular killed the appeal of microkernels. I think
they arrived at the result that microkernel systems are twice as slow as
traditional system (from my hazy memory).

The reason is that crossing the privilege barrier is expensive. Since each
service is implemented in its own process, they need to use IPC to communicate
with each other. IPC needs to enter the kernel, switch address spaces, and
return control to a different process. This is expensive because it enters slow
paths in the CPU (this also makes syscalls expensive), flushes tons of caches,
and compared to function calls, requires packing/unpacking arguments to an IPC
transfer buffer.

The cost of crossing was high back then, and it's even higher now. With the
Spectre "vulnerability" (which I think is hugely inflated compared to other
security issues), it's guaranteed that crossing of privilege barrier will
remain expensive, as CPU designers will make sure to flush EVERYTHING when
crossing.

With monolithic OSes like Linux, crossing is expensive enough that Linux added
"userspace" syscalls, with a mechanism called vDSO. For example, you can query
the clock without actually entering the kernel. That they went through this
effort shows that the performance penalty from crossing is generally bad.
Microkernels have to pay even more, because IPC involves address space switches,
and the equivalent of a single syscall on a monolithic OS may require multiple
IPCs (depending on design).

In addition, a monolithic kernel can freely access the calling processes'
address space. On a microkernel OS, a server process is "not supposed" to be
able to access the address space of a calling process (for effective privilege
separation), so you need additional (expensive and complex) mechanisms to deal
with that.

Imagine you're implementing UNIX pipe() on a microkernel. The microkernel is not
supposed to even know about UNIX pipes or FDs; you need to implement this in a
server process. A write() call would need to be implemented as IPC call to
that server, and the data to write would be copied with the IPC, stored in a
temporary buffer. When another process calls read(), another IPC needs to be
performed, including copying all the data again. In a typical case of a blocked
reader, a monolithic kernel could do this with 1 instead of 2 switches, and 1
instead of 2 copies. (There are some more gross details and implementation
strategies to be considered. It's just an example.)

Many monolithic OSes allow implementing some types of drivers/services in
userspace, but they're typically slower and sometimes "stunted", and can't do
everything a kernel driver can do (consider FUSE on Linux, or writing "drivers"
with libusb).

Another issue is complexity. Modularization is hard, especially if you need to
achieve performance and security. Consider the pipe example above (or anything
else including networking and normal filesystems) - there are tons of complex
things you could try to optimize them, all which add complexity. How do you
protect OS services against DoS? How do you protect other userspace OSes from
misbehaving servers (did you ever deal with a frozen FUSE sshfs on Linux)?

I find it important that they really failed to create a fully modularized
system with _useful_ privilege separation, and instead they usually end up with
some central servers (that are single point of failures both in terms of
robustness and security). Sometimes they admit this, like in this relatively
recent academic work about MINIX (and which I'll conveniently not cite), where
the author admitted that some of the robustness claims are sort of bullshit.

A true microkernel also implements drivers as userspace processes. Lots of
hardware doesn't even allow implementing "unprivileged" drivers, because the
hardware can access the entire system memory anyway, or there are inherent
problems like needing to deassert level-triggered interrupts on shared interrupt
lines.

Microkernels also were connected to distributed systems. Scientists thought we'd
have a single OS distributed over multiple network connected computers. One idea
was that a distributed OS is really the same as a microkernel OS, just that
network separation is a slightly more radical form of process separation. I
think we can safely declare these ideas as failed. Nowadays, distributed
computing means that you can run a bitcoin miner in an unsuspecting user's
browser.

One of the wildest claims of the microkernel hype was that a single system could
provide multiple OS "personalities". So you could run OS/2 (hey it was relevant
back then) and Unix on the same microkernel, simply because you could implement
them as userspace components. IBM actually tried this (and guess what, they
failed). If you look at projects like wine or WinNT Linux emulation, you'll
realize that you don't really need such a kernel to achieve your goal, as well
as the fact that implementing an entire OS on top of another one is a
"difficult" task.

In summary, anything is probably always going to be simpler and more efficient
in monolithic kernels. It's even possible that "logical" security issues (that
go beyond buffer overflows in server code) are easier to avoid in monolithic
kernels.

Microkernels 2nd generation (German engineering)
------------------------------------------------

In the 90ies, a researcher (Jochen Liedtke) recognized that one of the major
problems of microkernels was IPC performance, and tried to find out whether
IPC really had to be slow. He determined that IPC is inherently slow, but that
the Mach implementation (and others) was _much_ slower than necessary. He
created L4, a microkernel optimized for raw IPC. The original L4 (L3) paper is
kind of an amazing read, because it's so focused on its goal and has good
results to show for it:

    http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.4581

It made quite an impact, and apparently all kernels in this style are now
called 2nd generation microkernels.

This didn't really answer how to build microkernel OSes, but it did prove that
Mach (US/Californian technology) is crap. This sort of confuses me, because
researchers of all kinds have tried to fix Mach performance for at least a
decade. You'd think they'd have tried the same as Liedtke.

Hybrid kernels (aka even more microkernel failure)
--------------------------------------------------

Sometimes there is talk about "hybrid" kernels, that include the best of both
worlds. But it's really just marketing they used when they wanted a microkernel,
but had a monolithic kernel.

(On the other hand, maybe this is the right label for attempts to design
operating systems that try to implement UNIX in a privilege separated manner,
more of that below. Some early microkernels also might have been called hybrid
kernels, e.g. due to having drivers in the kernel.)

For example, WinNT is occasionally called a hybrid kernel, but it's really not.
It's a traditional, albeit modular kernel. At some point WinNT even implemented
font rendering in the kernel (JESUS FUCKING CHRIST). WinNT was created in the
early 90ies, where the microkernel hype was at its height, but it was also clear
that a working OS wouldn't be a microkernel.

Apple OSX (aka macOS and many other confusing spellings) has parts based on Mach
(well it still uses Mach), but it's really a monolithic kernel that's mostly
FreeBSD. I'm not sure, but it's probably implemented as "co-located FreeBSD
personality", which is bullshit speak for "awful disgusting chimera of FreeBSD
and Mach all in the kernel". It seems Mach IPC on that system is mostly used as
equivalent for D-Bus on Linux (or actually the other way around), but I don't
know enough about it to really tell.

As far as I know, Apple uses co-located servers. Normally, Mach servers run in
their own process (and own address space, for privilege separation). Co-located
servers are Mach servers that are located in the kernel's address-space, but
otherwise behave like true Mach servers. Things that use IPC on Mach typically
use MIG (Mach Interface Generator) to generate IPC stub code from IDL (or so).
That means a function generated by MIG looks like a normal function, but
actually performs a Mach IPC call (it hides tricky parts like packing and
unpacking arguments to an IPC transfer buffer). Co-located servers are built
with MIG turning certain IPC calls into plain procedure calls (or well, at least
skipping the IPC syscall), which makes them much faster. The claim in the paper
that introduced it was that microkernels were all about modularization all along
(duh!), and privilege separation is not important for central servers like the
UNIX personality (duh!).

In summary, Apple OSX is a monolithic kernel built upon a microkernel as base.
Note that Apple tried porting Linux to Mach before that (why did they even try
this), but failed: http://mklinux.org/graphics/dpenguin.gif

Google Fuchsia is a new OS that is called "microkernel" by some. As is typical
of Google, the only information available is in their repository, although they
do provide some design docs. There are no papers, no plans, no outside
involvement. It's just a source dump, and some journalists circle jerking around
it get their information from those source dumps. It might be a 1st generation
microkernel or some variant of a hybrid kernel. It's interesting that it's
based on an open source project (LK), and that its author was hired by Google
(whether for/to work on Fuchsia or not, I don't know). Based on general
situation of Google tech, it's probably either crap, or even if not, Google
won't care one bit about anyone but themselves.

(Offtopic: Why does Google open source stuff but not care about open source?)
-----------------------------------------------------------------------------

Fuchsia (see above) really drives home how Google does internal development and
how it relates to open source.

Even though Google is hailed as major open source company (or some such), it is
toxic to the open source ecosystem. There are 2 things they do:

    1. Participating in development of open source software they do not "own"
    2. Provide as much source code of their software as they can in public,
       repositories (including history, often accepting 3rd party patches),
    3. Participate in open standards development and The Web(tm).

There is nothing wrong with that, except the way how they do this.

First off, why do they use open source? Because it's an enormous quantity of
relatively high quality software that is available free of charge, usually even
with (essentially) waived copyright. People will maintain it for free, and even
fix bugs and implement features for them for free. (Just imagine it. One of the
biggest and richest companies on the world, and you do stuff for them for...
free?)

The first problem is that Google devs won't fail to use their "leverage" of
the company they work for to force in their changes. For example, they might
have a patch that is not ready or even against the community, but with their
name and influence, they will get the change in anyway. Why would a project
allow this? Because Google devs have enough energy to do it. The company is
behind them (there's little doubt the dev's manager is usually convinced he's
doing the right thing, would be an asshole of a manager otherwise). They can
just spend a lot of time to "convince" the other devs that it's really
absolutely necessary. Further, Google may hire more project members to tilt
the development generation into their favor.

Last but not least, they could "hard fork" the entire project, and marginalize
the original project. In particular, this would remove project members that had
been hired by Google before, and would potentially remove infrastructure
provided by Google. Depending on the open source project and Google's
"involvement", this could range from disadvantageous to fatal for it.

Most of the time it won't go this far of course - because everyone knows that
going against a big company (that became very involved with the project) is
probably a bad idea or will lead to "drama". And probably that it will mean to
lose their support and contributions. You don't just bite the hand that feeds
you. Google can exert influence without anything "bad" happening.

This dynamic is not necessarily something individual Google devs use on purpose
or are even aware of, but it's definitely something Google as a company as a
whole participates in. In summary, their involvement with open source is
characterized by a certain mostly subtle ruthlessness/inconsideration, that
doesn't put them into a too good light.

Small projects are often just "grabbed" and essentially hard-forked, without
anything ever being contributed back. They probably do it if it's convenient.
(Look into the third_party directory of any Google project. You could probably
research how much of that software has non-upstreamed patches.)

The second thing from above, providing source code of the projects they develop
internally. This affects software like Android and Chrome, and of course
Fuchsia. The truth is, these are not real open source projects. There is no
community. There is (apparently) not even access to whatever place Google devs
use to discuss development (just their git logs and maybe gerrit).

Essentially, these repositories are only source dumps. It's a bit puzzling why
Google would even bother to make them public, without trying to create a
community around it (which is required for companies to get anything out of
open source developed by themselves). You could come up with a number of reasons
to do so: Google makes public as much as they can so they can tighten security
about the really important bits, or to appear "cool" to young people, or to
curb copyright violation claims by willingly exposing internal development to
the public ("hide in plain sight", if you want to be mean), or maybe they really
expect drive by contributions under these circumstances. It could also be to
make outside developers to become interested in the technology they're
developing (interested developers may teach themselves about the technology
before they're hired, which is obviously very good for the company).

Some people even like Google's source dumps, because it's better than nothing.
If some Google library or API is undocumented, they may be able to look into
the source code to find out how to use them. This also helps Google without
having to invest additional effort.

I think it's really just that Google _knows_ that publishing their source has
only advantages. The part about being cool is a real thing; just look at
Microsoft's behavior currently. Developers developers developers.

But the main takeaway here is that Google doesn't give a shit about anyone but
themselves. Their open source is not participation and building software in a
community - it's just for getting forward faster, and ultimately for making
money faster. This is why I claim that their open source is "toxic". Don't you
dare to claim they do it out of altruism or some open source ideology thing.

And the main consequence is that you should be careful about using Google
software. They won't care about your use case. They won't care about your
contributions (at least not if they seem to go against Google's desired
direction of development). They won't care about your tools. One of the dumbest
Google software thing is that you need to use one of their shitty build systems.
They will also use their shitty build system to build outside dependencies. It's
really shitty. Also have you ever used depot_tools? Be relieved if you haven't.
Google is essentially an ecosystem in itself, that's why it doesn't truly fit
into the open source ecosystem.

Look at others trying to use Google software. In almost all cases it's pure
chaos. Google will build stuff for themselves, it doesn't matter whether others
can use it. This starts at a pretty early and obvious point: the build system
and deployment. They won't care whether their own changes break other people's
software or use cases. Using Google software is a high maintenance matter. Also,
Google maybe have a certain degree of engineer elitism, but there's still plenty
of room for Google engineers to produce pure crap.

Note that this isn't a black and white thing. Google does contribute useful
things to community projects, open source Google software can be useful, and
nobody wants to hurt you anyway. It also depends on the individual Google dev
and how he/she/it works with the community. But if you don't see a tendency,
then whatever.

I also don't claim to be a Google expert, this is just my experience and what
I heard from others, plus a good deal of approximation. It's not Google specific
either. Most big companies offend in similar ways, just that I find Google's
behavior the most obnoxious.

(Strangely, Google is also closing projects sometimes. Apparently more and more
Android OS parts are getting closed (certain "apps", etc.). Likely this is to
prevent minor changes to the software by effectively Google competitors. Some
company could simply create an advantage over Google or partners by adding an
additional minor feature to a builtin app. This is the phone market, after all.)

(Also, did you ever notice how Google is getting rid of GPL software? Why would
they dislike GPL if they open source everything they own anyway? The only reason
I can come up with is because they don't essentially own it, like they do with
BSD licensed stuff. Maybe additional "duties" attached to the license makes
their management nervous, and they used GPL projects out of pure necessity.)

Last but not least...

What about participating in open standards? Just recently, Google tried to free
us from the grip of the HEVC patent trolls, and created the AOM association.

HEVC is the successor of h264, the most important video codec. The x264 open
source implementation and the piracy scene associated with it showed how
efficient it could be. Digital video didn't have to be postage-stamp sized eye
cancer to be watched through proprietary garbage software. (Who remembers
RealPlayer? Better don't.) HEVC is supposed to be an important improvement over
h264 (especially for 4K and HDR, both buzzwords that won't actually improve
video quality one bit). Now, Google regrettably forced VP8 on the world (a bad
codec essentially retrieved from the Flash plugin), to avoid having to pay h264
licensing fees. VP8 was crap, so they created VP9 - a much better codec, which
mostly failed due to the Google encoder engineers being plain incompetent, and
for being a Google-only project with all the associated inconveniences for
outsiders (see above, Google doesn't make it easy to use their software).
Eventually they created AOM.

AOM is a consortium of multiple companies trying to create a patent free video
codec. The main focus was to create something as good as HEVC, but without
having to pay royalties to multiple patent trolls who are holding HEVC hostage.
(Like someone said, MPEG-LA, the licensing company managing payments for h264
and HEVC "flew too close to the sun".) This video codec is AV1.

And guess what, there's so much Google engineering involved that some...
observers are utterly disappointed with it. Especially relative to what they
expected from Daala and Thor, earlier non-Google video research projects that
later joined AOM for AV1. The AOM "reference" encoder and decoder is crap.
Turns out these tools are just based on libvpx, the tools for Google's VP8 and
VP9. With all the consequences.

(When VideoLan started work on an actually good decoder, dav1d, Google
apparently reacted salty at first. Later they funded and started to use it,
 though.)

Regarding the codec standardization itself, word has it that Google applied
different standard to their contributions and that of others. Google's stuff was
often developed internally, and then just dumped to the normal AOM development
process (and sometimes apparently just pushed to git circumventing the normal
review process). On the other hand, other contributors had to go through
convoluted review and test procedures. It's clear that Google just used their
weight. It became a Google project, instead of the community project some hoped
for.

Another interesting thing is what Google did to WebAssembly. (Just a thing I've
stumbled over randomly.) You can read about it here:

    http://troubles.md/posts/why-do-we-need-the-relooper-algorithm-again/

In summary, Google made sure WebAssembly would produce output that V8 (their
JavaScript engine) could use efficiently, while it would suck for everyone else.
Specifically, compiler need to bend upside down to produce web assembly, because
Google is ignoring a particular concept that has dominated compiler research for
decades. Google just ruthlessly forced their own bad shit one everyone so they
didn't have to move.

Nice company.

Microkernels today
------------------

I think there were some research OSes, which are all dead by now. GNU Hurd is
apparently a microkernel, but I don't know if it qualifies as real microkernel,
or if it has drivers in kernel space. Also, it's dead.

There is L4Linux, which is normal Linux ported to run on a L4 kernel. I'm not
really sure why they did this. For one, it was probably useful to evaluate IPC
performance in a real world setting in a meaningful way. They could simply
compare how much slower it ran than "native" Linux. Another thing is that you
get a full Linux system, with the possibility to run certain programs "outside"
of Linux in segregated L4 tasks, for the sake of realtime behavior and security.
This is probably great for certain embedded applications. If you really want to
know, their website has tons of academic papers to sift through.

Unsurprisingly, many uses of microkernels are in the embedded world, and
apparently mostly as some sort of secure hypervisor, with real OSes beneath
them. These are not "real" microkernel OSes, because of the limited scope of
their use cases. I suspect the rise of DRM will create actual reasons for
companies to care about "security", and adding a secure microkernel
("hypervisor" style) is the easiest way to do it.

Some projects, especially research or hobby OSes, also claim to be microkernels.
That's because microkernels are still cool, and at least wrt. research,
improving security (and thus privilege separation) is the only thing left to do.

As mentioned above, Fuchsia is a commercial OS project that is said to use a
microkernel.

Don't confuse microkernels with realtime OSes (RTOS). RTOSes are usually made to
be tiny, but often they lack the general purpose direction or privilege
separation mechanisms that are essential to microkernels. For most embedded
uses, microkernels are actually too heavy, and the services they do provide are
kind of useless.

What happened instead of microkernels
-------------------------------------

I think virtualization (virtual machines, containers, hypervisors, etc.)
essentially replaced microkernels. You want to isolate a process in some way
because it could be dangerous? Just run an entire OS in a VM! You want to
sandbox parts of your server? Just use containers! (Well, that's questionable,
but it's a typical reason given for using such setups.)

Apparently there was also progress in general software development that managed
to tame monolithic kernels: they're no longer crashy pieces of shit (most time).
It's pretty rare that Windows or Linux crashes. If you ask your grandpa, he'll
tell you that Win 9x used to crash at least once every day.

Nowadays, you can write userspace drivers in some cases, e.g. FUSE and libusb,
which fulfills another microkernel promise.

Linux keeps pushing for very weird stuff, like extending BPF (originally byte
code to customize network packet filtering in the kernel) to be more or less
general purpose sandboxed userspace code running in the kernel. I eagerly await
the day when Linux allows implementing entire filesystems or drivers in BPF,
just so I can laugh at the world. BPF seems to be mainly abused to circumvent
the expensive syscall barrier, though.

Are microkernels en-vogue again?
--------------------------------

Not really, but some things have changed since to 90ies. People are likely more
accepting of performance reduction for the promise of more security. A remaining
microkernel promise is that although privilege separation impacts performance,
it helps with security.

Computer security is an absolutely nonsensical chaotic horror. It's full of
nightmares like virus scanners and containers, both which reduce performance,
but which promise some security. Browsers these days use sandboxing by default,
which reduces performance, but is seen as necessary to isolate millions of lines
of potentially buggy and exploitable browser code from your system.

Basically, there is a push for creating sandboxed processes for all sorts of
things. If you did this with basic OS services, you'd have a microkernel. Of
course you will never reach a true microkernel OS this way.

What should OS research focus on?
---------------------------------

If you ask me (fortunately nobody does), trying to introduce privilege
separation. When I talked to Rich Felker (the most UNIX person I've ever
encountered), there does seem to be room for this, and the belief that UNIX
can be efficiently implemented on a, uh, tiny kernel. (Specifically, he
challenged my claim that it has to be inefficient - though I'm still skeptic.)
It all comes down to finer grained privilege separation, not microkernels as
general purpose kernel.

Microkernels have been traditionally general purpose. The idea is that they
should be neutral (policy free), so that arbitrary OS mechanisms can be
implemented on top of it. Once the microkernel is designed and implemented, its
development is finished, and the rest is up to the OS implemented on top of it.
The L4 inventor's most central claim was that microkernels must be small to be
efficient (and probably to guarantee other properties). Policy-freedom was
somehow critically connected to it (go read his paper if you're interested).

Is it possible that a kernel should really be designed to implement a subset of
POSIX, or specifically allow implementing POSIX with low overhead? Such a kernel
would still be relatively small (though probably much larger than a L4 style
kernel), but would violate the "policy free" property of microkernels. Thus, it
wouldn't be a microkernel. You'd have to use another label.

Why UNIX? UNIX is the most widespread OS API, and it does most things in pretty
straight-forward ways. Everything will have files and file handles. Everything
needs to implement read() and write(). Take Windows: it's just a very complex
take on UNIX, with some parts destroying the fundamental elegance of UNIX. (Like
making many things not behave like file handles - ever used Windows sockets,
the console API, or named pipes? If you know the UNIX equivalents you'll
probably agree with me, even if you dislike POSIX.) My claim is that UNIX is
the fundamental general purpose OS interface (if you go beyond extremely
embedded use cases), and there is no need for microkernels to be truly general.

The real questions are:

    1. How could such a system be designed?
    2. Could such a kernel be more efficient than a POSIX OS implemented on top
       of a true microkernel?
    3. Could such a kernel be as efficient as Linux or BSD?

The project in this repository is aiming to explore how to implement POSIX in a
way that leaves most icky POSIX stuff outside of the kernel, which in turn was
provoked by not liking some parts of POSIX, but recognizing the absolute
pointlessness of coming up with new interfaces. So so answering these questions
is not really what I'm aiming for, although it might be exploring 1. Also, the
reference required in 2 does not even exist.

Microkernel research projects partially try to answer the question of "could
a kernel built on top of a microkernel be as efficient as Linux" (so not really
1 to 3 from  above), see projects like L4Linux. But note that they do not try
to build _OSes_ on top of microkernels; L4Linux for example still has a kernel
process that can freely access the entire RAM mapped for L4Linux (including all
Linux userspace processes).

But in truth, this is probably all a past thing for OS researchers. All of this
was already considered, and the proposed idea of looking at high performance
real world systems AGAIN will probably appear restrictive outdated. They're
looking at much spiffier things now. And that's why we still use UNIX variants
(mostly Linux), and (ugh) WinNT.

Read "Systems Software Research is Irrelevant":

    http://doc.cat-v.org/bell_labs/utah2000/utah2000.html

Read it again every other decade. We're (re)making old stuff here!

References and summaries of systems (whether working or proof of concepts) that
do 1. are missing. Maybe at least Plan 9 deserves a mention.

IPC
===

IPC enables communication between two processes. Thus its name: Inter Process
Communication (what a surprise). On UNIX, you get pipes, unix domain sockets,
shared memory, FUTEXEs (over shared memory), and some other things.

You could build a UNIX system with fine grained privilege separation (e.g. each
filesystem in a separate process) on top of these mechanisms. Is it possible
that other IPC mechanisms could improve performance or ease of implementation?
I have no idea, so let's write down what mechanisms could be considered. An
academic paper would make this reasonably complete and give performance numbers
or credible estimates, but this isn't a paper, so there's nothing of that.

My intention is to list some hopefully representative examples.

Basic assumptions
-----------------

Usually, we want to perform RPC over IPC. RPC (remote procedure call) implies
sending a message/packet to the other process, and usually waiting on some sort
of reply. The reply could be asynchronous or optional.

If the messages are small and a reply is required, the overhead of the IPC
mechanism to send 1 message will matter. If there are small messages without
reply (or asynchronous replies), multiple messages could be batched, possibly
reducing IPC overhead. For very large messages, the copying of the message data
itself likely matters most for performance.

So you can put IPC transfers into 3 categories:

    1. Small messages & synchronous reply => RPC-like
    2. Small messages & async. or no reply => message batches
    3. Large messages (with any type of reply) => large transfers

It's possible that you'd implement 3. as 1. or 2. with a different IPC mechanism
to handle transfer of large payloads.

Shared memory & FUTEXes message transfer
----------------------------------------

The most primitive, and probably in many cases fastest kind of IPC is shared
memory. For message transfer, you need to combine this with a synchronization
and notification mechanism, e.g. mutexes and condition variables. These can be
done with process shared mutexes (using FUTEXes, this can be almost as simple
and efficient as process local ones). Usually you'd implement a ring buffer,
basically reimplementing a UNIX pipe in userspace.

FUTEXes are locking primitives that can be used to implement mutexes and
condition variables efficiently. In particular, they make expensive kernel
entry optional in important cases. (FUTEX is the name Linux chose, though the
concept can be found in other operating systems.)

This requires some level of trust between the two processes. One process could
always deadlock the other, or write non-sense to the shared memory area and
corrupt control data, or subvert validation of message data the reader might
have done in-place.

Advantages:

    - For small messages probably the most efficient mechanism, especially if
      message batching can be done.
    - In the best case the number of kernel entries and context switches
      approaches minimal: Uncontended locks require no kernel entry. A
      notification through a condition variable will wakeup the other process as
      soon as the associated mutex is unlocked. When sending a message the
      reader could immediately start work without further kernel entry. Not sure
      if the writer could put itself to sleep at the same time as the reader is
      woken up, or if there's going to be some scheduling overhead.

Disadvantages:

    - Complex communication protocol. It requires defining how to manage the
      ringbuffer and synchronization. Likely you'd use a shared library for
      handling the details.
    - Requires choosing an adequate size for the ring buffer (or have a complex
      buffer resizing protocol).
    - Can require up to 3 copies. If one process doesn't trust the other, the
      data needs to be copied to process-local memory to validate its contents.
      In the best case you can construct the message directly in the shared
      memory, and the reader can process it directly, which I consider 1 copy.
    - Inadequate for large data. Of course large data could be transferred via
      separate shared memory regions created for this purpose.
    - Only point-to-point. Having multiple readers/writers would raise
      complexity and impair performance, or may require creating new shared
      memory areas for each "connection".


Pipes, unix domain sockets
--------------------------

Pipes transport data over process boundaries using endpoints given by a pair
of UNIX FDs (file descriptors, process local integer IDs for a conceptual file).
You write() data to one end, and can read() it from the other. poll() (and
select() etc.) can be used to check presence of data.

Unix domain sockets are similar to TCP networking, except they're strictly
local. They use the UNIX connection oriented socket API, so code using it can
look extremely similar to code using TCP, except nothing will go through the
network stack. Instead, Unix domain socket connections are more akin to
bidirectional pipes, and likely as efficient.

Since pipes are byte streams, message framing and parsing is required. Some
operating systems have hacks to enable message transfers over pipes (I'm
referring to Linux pipe2() O_DIRECT). Unix domain sockets support proper message
passing with SOCK_SEQPACKET (there's also SOCK_DGRAM, but that behaves too much
like UDP).

Advantages:

    - Simple setup, trivial protocol at least with packet oriented streams.
    - If you use byte oriented streams, you can send and receive multiple
      messages with 1 syscall (good for message batching).
    - If you use packet oriented streams, you may be able to invent syscalls
      that achieve the same (e.g. see Linux sendmmsg()). For receiving it may
      become tricky to handle large packet sizes (incremental receiving of
      packets sounds like a mess).
    - The kernel may apply zero-copy VM tricks when large messages are
      transferred (such as sharing physical pages as COW).
    - In the best case, it could work with 1 copy: the message can be
      constructed anywhere and passed to send() directly (i.e. not counting as
      a copy), and the kernel could copy this directly to the receive buffer.
      You can use scatter/gather (iovec) to avoid having to copy different parts
      of a message into a linear buffer.

Disadvantages:

    - Additional kernel entry is needed to wait for message (poll() call).
    - Lots of scheduling and syscall overhead: send() + poll() + recv() in a
      typical message processing loop. But you could introduce "fused" syscalls
      that minimize kernel entry.
    - In some designs you may need to provide enough receive buffer for the
      worst case message size.
    - You may need to care about the pipe buffer and its effect on performance
      and system memory usage.

Note that on some OSes, unix domain sockets provide additional features like
passing FDs from one process to another.

A kernel could either implement the UNIX API directly, or provide a light-weight
variant of them without UNIX dependencies.

L4 style message passing
------------------------

Most microkernels provide dedicated message passing primitives, that don't have
dependencies on UNIX (like pipes/unix domain sockets), and that may optimize
message passing for performance, and provide additional microkernel features.
This subsection discusses the L4 variant.

L4 IPC was aggressively optimized for performance. It gets its speed from being
inherently synchronous and depending only on extremely simple other concepts.
For example, instead of FDs, which are usually implemented as an index to an
array of pointers to an internal file structure, as target for the IPC
operation, a global thread ID is used. The thread ID in turn can be directly
used to compute the address of the kernel's thread struct.

The IPC operation can perform sending and receiving in 1 syscall. This minimizes
the number of kernel entries for typical RPCs and message processing loops:

    - A call to a server performs an IPC send operation to the server. The
      arguments for the same syscall instruct the kernel to switch to receive
      mode. It sets the server's thread ID as the only allowed sender, in order
      to receive the server's reply.
      In the best case (if the server thread is in receive mode, and no error
      happens), the kernel will switch directly to the server thread. The
      scheduler does not need to be involved, because the original thread is
      blocked anyway.
    - The server gets the caller's thread ID from the IPC syscall, and will
      use it as sender target when sending the reply. The reply syscall will
      also instruct the kernel to switch to receive mode (to service further
      calls from other threads).
      In the best case (caller still waiting, no other threads want to send
      anything to the server thread right now), the scheduler will not be
      involved again.
      The reply will usually be done with 0 timeout, because the client is
      assumed to be waiting. Using a higher timeout could DoS the server.

So a call + reply can be performed in 2 syscalls (1 in each thread). The kernel
needs to access a minimum amount of data structures, and the work is essentially
reduced to a simple context switch.

L4 message passing further allows copying multiple strings (basically like
iovec on UNIX), as well as complicated page mapping operations. The latter are
essential for memory management.

It seems that L4 justifies the relatively complex payload options with the need
to have a single IPC mechanism, as well as the fact  that the syscall entry and
address space switching operations are so expensive, that using additional
syscalls and thus kernel entries would reduce the performance further.

It's not really clear how practical zero-copy copies would be implemented. The
L4 kernel has no virtual memory subsystem. It only has a mapping database to
support the IPC page map/grant mechanism, which can be used to implement virtual
memory. It probably means userspace has to explicitly implement COW, and the
kernel can _not_ for example transparently turn large string copies into zero-
copy COW'ed pages.

The exact details are probably best taken from various L4 papers and manuals,
since I wrote most of this out of my memory. There are many L4 variants too,
which slightly change the API around (minor details like platform specifics or
making the API portable, and major things like security).

Advantages:

    - You probably need this design to reach the lower bounds on syscall
      performance. Small calls can even be done in registers only, without
      transfers.
    - It's simple to implement.

Disadvantages:

    - Global thread IDs could leak tons of internal information about the kernel
      and the OS.
    - Access control needs to be implemented in userspace, since anyone can send
      IPC to anyone (attackers could even guess thread IDs). The "clans & chiefs"
      concept provided some coarse control, but it sucked.
    - Connection management must be implemented in userspace. You need to verify
      which rights a new unknown sender thread ID has.
    - Load balancing must be implemented in userspace. A thread can't just wait
      as proxy for another thread, so doing multithreaded servers is hard.
    - All in all there is non-trivial work to be done in userspace.
    - Very "tight" IPC syscall arguments, likely needs a stub generator for
      comfortable use, especially if you want to use the message buffer for
      payload data.
    - It seems it's implied that string copy source/destination are always in
      present pages; it's not clear what happens if they need to be paged in
      first (this is important for usability and whether clients can block
      servers via the pager).
    - Requires a protocol on top of it for message batching.

The primitiveness of the IPC means usually you will need to implement several
features that other IPC mechanisms like unix domain sockets provide normally.
Some of them look rather tricky: load balancing would require knowing a list
of server thread IDs (and then trying them all until one works?), access control
gets into trouble if a client creates a new thread and then makes a server call
from it (does the server need to ask some kind of management server to which
process the new unknown thread ID belongs to?).

It's notable that there was a project to port Hurd to L4. They gave up in the
early stages. I don't know why; maybe the primitive IPC mechanism created too
much complexity, and they started considering other L4 variants and kernels.
(This is not an academic paper, so vague hearsay from hazy memory and unfounded
speculation is fine.)

In my own lowly opinion, this focuses too much on raw IPC performance and
benchmarks, while a complete multiserver system may actually suffer from some of
the design choices.

Mach style message passing
--------------------------

Not fully describing it here, go read some paper.

It seems they provide something akin to a light-weight variant of unix domain
sockets multiplexed into one syscall (very broadly speaking). Instead of UNIX
FDs, it uses "ports" as target (similar to FDs, but refer to IPC targets). Ports
can be transferred via IPC. Call/reply idioms are supported. Copies may be
optimized with COW, and there are features for explicitly transferring pages of
memory to the receiver. The low level API seems sort of annoying, though.

See e.g.:

    https://www.gnu.org/software/hurd/gnumach-doc/Mach-Message-Call.html
    https://web.mit.edu/darwin/src/modules/xnu/osfmk/man/mach_msg.html

There are multiple versions of Mach, which might matter, so care should be taken
when looking at random web resources. (I seem to remember that OSFMK/XNU/OSX is
based on an older Mach iteration.)

Thread migrating IPC
--------------------

Mach IPC was slow. One (apparently somewhat successful) optimization (based on
earlier research) is thread migrating IPC. The idea is that instead of passing
a message from thread to thread, the message should be processed in the sender
thread. The thread will be migrated to the target process (changing address
space, and necessarily the stack), continue executing there, and return to the
caller process when sending the reply.

It's not clear why this is faster. Maybe Mach had severe overhead in specific
areas such as scheduling. The new mechanism also enforces synchronous IPC. It
requires "activations", essentially a free list of userspace stacks, so
arbitrary caller threads can come into existence in target processes. These are
almost like pre-created user threads without kernel threads. It seems from the
Mach perspective, activations are just light-weight target threads, which
dramatically increase performance due to excessive inefficiency when handling
normal threads.

In conclusion, this is somewhat similar to normal message passing, instead of
popping a waiter thread from a port's wait list and switching to it, you pop an
activation and switch only the user thread.

Advantages (relative to normal message passing):

    - Enforces synchronous IPC.
    - May reduce the number of kernel threads (and save memory and caches at
      least), and reduces kernel thread context switches.
    - Nothing?

Disadvantages:

    - Needs to maintain a kernel migration "stack" of processes a thread
      migrated though.
    - Very rigid and enforces the call/reply idiom.
    - Weird problems due to not being able to return control to a caller thread
      without aborting all calls: what if a UNIX signal happens in the caller
      thread? What if one process in the migration stack gets killed? This may
      be too inflexible.

Kernel command ringbuffer
-------------------------

As claimed further up in this blog, shared memory is the most efficient manner
of communication if you want small batched messages. Unfortunately, there are
also security issues. The main reason for the speed is (possibly) reduced
copying, and reduced kernel entry.

Instead of shared memory, you could setup a ring buffer that is read and
"interpreted" by the kernel. The kernel, once notified, would read the buffer,
and perform the operations that were written to it (a concept that's constantly
reused, see e.g. https://lwn.net/Articles/776703/).

This could for example be a list IPC operations. You could batch messages this
way, even if the individual calls are traditional single-message calls. Since
the kernel interprets this, the commands can have argument pointers to arbitrary
other memory outside of the command buffer.

memfd (large memory transfers)
------------------------------

Linux memfds are UNIX file handles to a shared memory area. This shared memory
does not have to be mapped anywhere (not does it need to be in /dev/shm/), but
can be mapped by using mmap() on the FD.

Classic fbufs also seem related. dmabuf is a similar Linux mechanism, but
apparently for physical memory.

Most importantly, a memfd can be made read-only, and you can send it over unix
domain sockets. This is interesting for zero-copy transfers of large data.
Unlike with zero-copy copy optimizations and explicit memory management (like L4
and Mach provide them), this does not necessarily involve manipulation of page
table entries and TLB flushing, which makes it an interesting concept in
general.

Discussion
----------

What is there to discuss? It's not a paper.

My own thoughts and taste is reflected in the section below.

IPC implementation
==================

Coming up with IPC requires a bunch of annoying design decisions. Not only does
the concept and the API need to be decided. You also need to have an idea how
to efficiently and easily implement it, and its impact on the overall OS design.
The purpose of the IPC is to be useful. All aspects need to be considered
together.

If nothing else, this section documents my train of thoughts, and how I arrived
at the garbage in this repository. For myself, this is always a balance between
spiffy crackpot ideas that are useless in practice (in my honest opinion), and
the simple, reasonable, efficient thing, but which are kind of boring.

Special IPC abstractions instead of UNIX read/write
---------------------------------------------------

I'm going to create separate microkernel style kernel abstractions for IPC,
instead of making the kernel support unix domain sockets directly. This gives
more freedom experimenting with IPC designs and doesn't require pipe buffers in
the kernel.

Unix domain sockets could just be implemented on top of this IPC mechanism.
Although it's somewhat likely that won't work. You probably need a kernel ring-
buffer  to provide all expected semantics, which the IPC won't provide. Well,
whatever.

This probably also means there won't be a VFS layer in the kernel.

Combining send/receive
----------------------

To reduce kernel entry overhead, sending and receiving should be possible in
one operation. This also avoids scheduler overhead in the best case.

In a broader sense, batching multiple syscalls or IPC operations into one kernel
entry may be useful, but that is hard to use with the anticipated use case of
implementing RPC (need to wait for a reply on each send), and could be premature
or just useless optimization. On the other hand, combining send/receive is the
minimum you can do, and it's easy to do.

IPC port abstraction
--------------------

IPC endpoints are denoted by process-local identifiers similar to UNIX FDs. They
can be transported over IPC themselves. Compare to L4 this requires an
indirection through a lookup table.

(In an earlier experiment, I tried to put this table at a fixed kernel address
that are process specific (MMU_FLAG_PS), but it was obnoxious to deal with, and
likely wastes TLBs.)

(The kernel won't have global identifiers. Although note that implementing
process-shared locking primitives based on FUTEXes need globally unique thread
IDs anyway. So the UNIX server (ugh) will probably generate these numbers and
pass them to the threads it manages. Stupid.)

In addition, these port handles can store a user data pointer for the server.
The server can use this to avoid an additional lookup. In fact, the IPC receive
operation returns only the userdata used at port creation, not a port handle.

I'm hardcoding IPC to enforce "call" semantics into one direction (sending a
message through a port, and waiting for a reply). There are 3 types of ports:

    1. Listener ports. They are for servers. You can receive from them only.
    2. Client ports. They are created from listener ports. You can send from
       them only, but you can wait and receive a reply as part of the send
       process. Client ports are created by the server. They are created from a
       listener port, and take a server userdata value as argument.
       Client ports are sent to a client, which can use it to send messages to
       a server.
    3. Reply ports. They are temporarily created when a server receives a
       message from a client, and are used to send a reply.

At least client ports can be freely sent to other processes. fork() or UNIX FD
passing may rely on this.

It goes something like this:

::

            Server                          Client
           --------                        --------

        - create listener port
        - create client port
          (listener port and userdata
           as arguments)
        - send port to client (through
          pre-established means)
                                        - receive port
                                        - send message to port, wait for reply
        - start receiving from listener
          port
        - receive message from client
          - this creates a temporary       ... client wais ...
            reply port
          - and returns the userdata to
            the server

           ... server works ...

        - server sends reply by sending
          to the reply port
                                        - client receives reply


Clearly this is meant for RPC. Possibly too inflexible in general. It also
requires an asymmetric IPC implementation: different code for "calls" and
"replies".

Port revocation?
----------------

A capability based system would offer a way to revoke a port (so a client can't
use it anymore to send or receive message through it). Revocation would require
tracking all ports created from an original port, which is kind of annoying as
an implementation without that seems simpler and more efficient. A server could
in theory just reject requests from a "revoked" port instead.

Multiple waiter/sender threads
------------------------------

Multiple threads shall be able to wait on a single listener port. This trivially
enables multithreaded servers. (In fact, it looks like a server would only
create one listener port ever, and just fire up multiple worker threads to wait
on it.) Without this, it'd be awkward to balance client ports over multiple
threads dynamically.

Multiple threads shall be able to send to the same client port, independently.
You can access a single FD in UNIX concurrently (at least if you use pread and
pwrite), and the same shall be possible with IPC ports. This requires client
ports to be stateless, and necessitates dynamic creation of reply ports.

Dynamic creation of reply ports
-------------------------------

Reply ports are created when a client sends to a server, and destroyed when a
reply is sent through them. I see this as only way to support multiple sender
and receivers through a single client port. Fortunately, allocating and freeing
reply ports should be fast, and can be further sped up by caching reply ports
per listener thread.

Migrating threads? Not even going to try it.
--------------------------------------------

Implementing migrating threads requires creating "activations", which are really
just userspace threads without kernel thread. To interact nicely with the libc,
you'd need to create a normal userspace thread, and then destroy the kernel
thread separately.

The fact that you can't just cancel an ongoing RPC would probably cause some
major headaches.

Conceptually, migrating threads are somewhat attractive and simple. But in fact
L4 style synchronous message passing is probably as fast and simple.

ASM-only fast path and register-only message passing
----------------------------------------------------

As part of the experiment, there shall be a fast path, and the possibility to
transfer messages through registers only. This is probably worthless.

Transferring parts of the message through registers complicates stub code and
probably necessitates a complicated stub generator. You could create a userspace
wrapper, that puts the start of a message into the registers, this would be much
slower than just having the kernel copying everything directly. Even if you do
have a stub generator, it might not matter.

The ASM fast path is supposed to prove that the IPC mechanism can be implemented
efficiently. The current (untested) implementation can actually transfer 8
registers without touching them at all. (Not that hard considering RISC-V has 31
general purpose registers.)

This causes lots of weirdness and is pointless since I

    1. Don't want to have stub generators.
    2. Am not going to make performance measurements or further experiments to
       prove whether IPC is really fast.

I hope I can bring myself to remove this.

Timeouts?
---------

Not having timeouts in the IPC operations seems to enable a more efficient and
simple implementation. Timeouts are probably rarely going to be needed. The
alternative is to create a syscall that can interrupt a waiting thread (UNIX
signal emulation would probably require something similar anyway). Then timeouts
can be done with a separate watchdog thread.

Complicated copying methods?
----------------------------

I have my doubts whether these are really useful. Normally you'd use other
methods for passing large quantities of data, like memfd.

I'll argue that iovec style copying as well as zero-copy optimizations could be
skipped. Instead, they should be implemented for memfds.

IPC implementation, alternative take
====================================

What if we directly simulate unix domain sockets instead? You can choose on a
spectrum from basically using POSIX semantics, to a vaguely similar mechanism
like Mach message passing (in the broadest sense all of them are asynchronous
connection and packet oriented transports with a kernel message queue).

The idea here is:

    1. Have a "connection" (socket FD) between each client and server.
    2. Client and server can freely send messages in either direction.
    3. Concept how to handle notification/waiting with minimized kernel entry.
    4. Connection creation is out of scope.

Optionally, this could be without a kernel buffer, but it would break poll().
You'd need synchronous "rendezvous" of 2 threads to perform a transfer, and
poll() cannot signal this condition.

Central to getting efficient IPC (premature optimization again...) is that you
can multiplex all these calls into one:

    1. Send operation at the start of the call
    2. Listening to multiple connections at once (kqueue()) afterward
    3. If a connection becomes ready, reading from it and returning

Something like::

    int listener = sys_create_listener();
    sys_listener_add_connection(listener, conn1, userdata1);
    sys_listener_add_connection(listener, conn2, userdata2);
    // ...
    while (1) {
        sys_msg next_to_send[];
        sys_msg next_received[];
        int num_send = ..., num_receive = ...;
        sys_ipc(&next_received, num_receive, &next_to_send, num_send, listener);
        if (next_received.valid) {
            void *userdata = next_received.userdata; // userdata1 or ...2
            ...
        }
    }

Maybe? It merges sendmmsg(), kqueue(), recvmmsg() (yes these are all Linux and
BSD specific, not POSIX).

A sender could do something similar for the call idiom (send/wait/receive).

Bad: multiple threads have a hard time to send requests to the server
concurrently if the connection is truly shared. On the other hand, there's no
easy way to "duplicate" the connection. Even added such a mechanism (which could
be used to implement dup()), there's still the question how to handle concurrent
accesses to the same FD: e.g. two threads calling read() and stat() on a FD,
how are the replies correctly routed? One could introduce a "sub stream
selection ID" that the server would use to distinguish them, but that's not a
very elegant addition on top of the POSIX semantics. There is no way to "lock"
the connection while waiting for the server reply either.

In addition, there is the question how simple FS calls should be serialized over
the connection. UNIX itself has no concept of transferring calls like seek or
truncate over network connection. Even plain sendmsg() calls involve pretty
complicated features, where it's questionable how exactly to pass it to the
receiver. So you'd need to do something "special" on the receiver side. An
obvious idea would be translating these calls into messages with a standard
format (isn't that what Plan 9 does?), but then what decides whether it works
like a socket, or whether it magically translates calls?

It sounds simpler to have an IPC mechanism an abstraction layer below unix
domain sockets. Trying to use unix domain sockets directly for this layer
probably sucks. My idea to have a lower level IPC mechanism to automatically
add a temporary reply port to send the reply to the correct caller seems
therefore more attractive to me.

TODO: add good ideas here

Implementing file access
========================

Filesystems shall be outside of the kernel, in separate processes. A naive
implementation would turn all write()/read() calls into RPCs. This would
probably be inefficient due to switching address space and twice the number
of kernel entries caused by IPC.

It looks like it's best if the kernel has a virtual memory subsystem, and
files are represented as kernel objects. Let's call them memobjs. Memobjs do not
have actual read/write entrypoints. Instead, they're a sparse linear memory
region comprised of pages, and read/write syscalls access this memory region. If
a page is not present, the actual filesystem is notified, and adds the page to
the memobj. If system memory gets full, or the file is flushed, the actual
filesystem is notified, and starts writing back pages marked as dirty.

Similar mechanisms have existed before (memfd, fbuf, maybe Mach VM stuff).

Special files (like network sockets, device files, /proc/, etc.) can't be
implemented with this and would need alternative mechanisms. Thus the memobj
file-like syscalls won't become the canonical file access syscalls. Rather,
userspace will need to switch between methods depending on the type of the file.

memobj API
----------

Such a memobj object requires the following mechanisms:

    1. read/write/mmap/truncate APIs, that vaguely work like POSIX.
    2. And probably also lseek() because even though it's annoying, you won't
       emulate that in userspace. The file position can't be in the memobj
       directly, and it can't be in the FD either. Instead it must be in an
       additional object referencing it, possibly together with file access
       rights.
    3. A notification mechanism from memobj to FS server when a user accesses a
       missing page.
    4. A way to query dirty/accessed bits for each page from the memobj. The FS
       server uses this to manage its cache and to decide whether pages need to
       be written back.
    5. Specifically the FS server needs ways to atomically read/write pages from
       the memobj. For example, if the FS server writes back a dirty page, it
       must be ensured that concurrent write operations to the page by clients
       are handled in a meaningful manner. This includes an operation to remove
       pages from the memobj. Some of these could probably be controlled with
       specific read/write flags, gated by special FS server access rights.
    6. If it's intended that FS servers can read/write to a memobj page in-place
       (e.g. via DMA), a page locking mechanism is needed. On the other hand,
       maybe "transferring" a full page is sufficient.

Note that read/write can apply COW zero-copy tricks, and can just move pages
from/to the kernel internal object. In particular, the filesystem implementation

In the worst case, this would be slower than straight read/write RPCs. Consider
accesses which filly read/write files. On the other hand, it likely simplifies
some aspects of the system (implementing mmap() becomes trivial), and handling
of system memory pressure (maybe).

FS server implementation details for opened files
-------------------------------------------------

Opened files simply exist as membufs, nothing further. They are maintained by
the FS server, and clients have less privileged references to it.

On the example of the FS server wanting to flush a page to disk:

    - The FS server queries a list of dirty pages from the memobj, and decides
      to write one of them to disk.
    - It uses read() on the memobj, with a special "reset dirty bit" flag.
    - The read() is page aligned, and the MMU code simply rewrites the memobj
      page a COW inplace of the destination buffer.
    - The dirty bit is reset atomically.
    - The read() call returns, and the FS server passes it to the disk driver
      code (or whatever).

An alternative implementation would mmap() the membuf, invoke a special page
lock operation to block access for anyone but the FS server, and then pass the
address of the mmap'ed page to the disk driver code.

A read operation would work roughly similar:

    - The FS server receives a notification from the kernel that a page was
      requested by a memobj user. (The notification may use IPC or such, but
      the originator is the kernel's memobj code.)
    - The FS server reads the page by calling the disk driver,
    - and then uses write() on the memobj to transfer the page to the memobj's
      internal buffer (this ideally uses zero-copy).
    - The client probably wakes up due to the kernel code snooping the write.

ftruncate() calls would simply resize the membuf internal buffer. But the FS
server probably needs to be notified of this in some way. To a lesser degree
the FS server would probably also want to know when access/dirty bits are set
in order to update file times. Not sure if all these things could reasonably be
done if the FS server is forced to poll.

Implementing pipes and unix domain sockets
==========================================

My proposed native IPC mechanism (synchronous message passing) is unsuited for
implementing pipes directly between two processes. You could use a "pipe server"
that shuffles data between two processes, but that's slow and complex, so such
an approach won't be considered.

What makes pipes tricky is that:

    1. They have an internal buffer, so they can be asynchronous.
    2. Non-blocking access is possible, and a misbehaving client cannot block
       a carefully designed server, and the same is true the other way around.
    3. Needs to support things like poll() too.

Synchronous message passing inherently requires the two processes to cooperate
(not possible because they may not trust each other), or a third party between
them (I've rejected this kind of implementation). Consequently, it looks like
this needs to be implemented in the kernel.

Could it be implemented in userspace without a "pape server"? Maybe. You could
create a shared memory buffer (tricky to make safe and DoS-free), or you could
create separate "transfer" threads per pipe. The latter would perform the
transfer from a buffer in the sender address space when the receiver wants to
read. But in addition to high overhead, it'd probably be rather hard to provide
all POSIX semantics. For example, multiple processes can write to a pipe or read
from it.

For an in-kernel implementation, quite possibly a variation of membuf could be
used, which may help to reduce duplication of APIs and implementation. But this
question remains unanswered.

Descent 2 custom level reviews
==============================

Descent 2 may be an old game, but it's still fun to play. Things that
facilitated this is that it's open source (restrictive license, but still), and
the availability of a large number of custom missions. Most missions consist of
only 1 level (and a sizable number are just multi-player battle arenas), but
some missions are large and rival or surpass the original game.

Of course you still need the original game data to play them.

All of the missions below are single-player. They are in no specific order.

The Entropy Experiment
----------------------

A simple 4 level mission. Nothing amazing, but I think it has quite good level
design and robust Descent-like gameplay. I especially like level 3. Level 4 on
the other hand is dragged down by adding some weird robots, and the end level
boss.

Descent: The Enemy Within
-------------------------

Pretty good and large (26 levels) mission. Creative diversity of level designs.
It uses custom bots exclusively, but doesn't suffer from it. Notable high points
include level 15 (pretty well suited to play standalone), level 19 (includes a
tricky secret level), level 23, and level 26 (the last level). Maybe level 25
is a bit too hard and it's regrettable that level 26 doesn't come before it.
The boss robot in level 25 in it is pretty mean; be aware that it is partly
invulnerable like the last boss of the D2 default mission. Not sure if it's
possible to beat the level 26 boss. Level 9 has 1 or 2 rooms that will make you
think "I didn't know the D2 engine could do this".

Descent: Die Hard / Fight For Your Life
---------------------------------------

These are 2 big missions with 19 and 27 levels each. They are by the same author
and quite similar. These levels modify the usual gameplay a bit: you get a LOT
of weapons and can throw them at a LOT of robots. Usually, Descent tries to make
it hard by making powerful weapons scarce. Don't worry about that here. There
are probably more missiles and Vulcan ammo than you can use. Earthshaker
missiles are everywhere. Rather than solving puzzles, the levels are mostly
about creating explosions. There are some interesting traps as well. It's pretty
fun. Also a warning: it's best to keep a savegame of the level start. Sometimes
there are inescapable situations or bugs that trap you in closed sections.

The level count is in part inflated by the number of boss arena levels. These
are relatively simply levels with huge spaces and multiple boss robots. The
latter is not really something supported by the game (there is a normally
disabled Assert() against multiple bosses), but it sort of works in a glitchy
way. As a consequence you only need to beat one of the bosses, and the others
will enter "death roll" as well. Normally I hate boss fights, but since you can
just pick the easiest boss it's not so bad.

EQ's Sesame Set
---------------

This is apparently a loose collection of 12 unrelated levels. The levels range
from sort of boring (sorry) to extremely interesting. Level 1 starts with a
really weird design: it's just one extremely big block (technically 2 segments
I think) where a large number of robots are placed. Level 3 is an interesting
puzzle, just that the level designer apparently forgot to place robots (?),
and there's only a reactor and lots of weapons that can't be used. But the
puzzles are very hard (inescapable traps at some points). Level 8 is one of
my favorite levels, because it's so strange. Level 9 continues aspects of the
level 8 level design, and add a load of strangeness on top of it. Both level 8
and 9 are very strange (what's up with these textures?), large, at times
puzzling. They're so good that levels 10-12 are unfortunately disappointing. In
any case, worth playing.

Plutionian Shores
-----------------

30 levels, relatively standard, but good. Apparently quite recently released.
It's nice that there are still people making large missions 20 years after the
game has been released. (Two fucking decades!) A bit annoying that you have
Ammo rack and Gauss canon only by level 15 (and the level sure makes you work
for it - you have to take a detour into the fancy secret level).

Descent 1: The Lost Levels
--------------------------

A nice long mission (24 levels). Not sure why it has "Descent 1" in the title.
Technically, it's a D2 mission, and uses plenty of D2 robots (but also D1 robots
and custom ones). It's mostly normal D2 level design and gameplay. My only
complaint is that it's too hard in the earlier levels while it becomes too easy
later on. (On "Ace" difficulty level I needed to save & restore all the time in
the early levels. On "Hotshot", it was too easy and boring in the later levels.)

Saturn
------

6 levels, but at times quite amazing level design. The tunnel in level 3 is so
interesting, and level 5 has one of the best "outdoor" levels I've seen. Some
hard puzzles.

Brat's Maze
-----------

No idea who Brat is. This is a 5 level D1 mission (D2 can load it anyway). I
think level 2 and 5 are pretty amazing. Level 2 for its well-made hectic game
play on small space, and level 5 for its visual design (it's boring otherwise).
The other levels are just OK. The whole thing is worth playing.

Jonadab's Domain
----------------

No idea who Jonadab is. This is a D1 mission again, with 11 levels. This is
an "experimental" mission, with small but interesting levels, to the point of
using engine glitches. The unusual ideas and level design make it interesting.

Descent 2: First Strike
-----------------------

This is apparently a conversion of the Descent 1 single-player mission. It has
been "spiked" up with D2 features and robots. (There's another Descent 1
conversion mission that is apparently truer to Descent 1.) Even the level design
is enhanced in some places with additional tunnels. Can be recommended.
