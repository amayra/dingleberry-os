This is a small hobby kernel. Is it useful for anything? Not at all. Purpose is
a combination of ergotherapy and self-harm.

But what does it do? It prints "Hello world." to the qemu serial console. In
case I keep developing on this, the way it prints this message will become
increasingly more complex (but still "Hello world." on a serial console).

Blog
====

In the following sections I'll blog about random crap because nobody cares.

What is hobby OS development
============================

Pointless.

Ah, let's try again...

Hobby OS development deals with the exploration of OS concepts, in particular
the kernel. People start by implementing the kernel, and if they get as far,
maybe a bunch of drivers and userspace parts. You can focus on how to implement
kernel mechanisms (threads, mutexes, etc.), maybe how to implement the
equivalents in userspace on top of the kernel. Or you can focus on operating
system concepts, for example microkernel vs. something else, or concrete stuff,
like how to design the filesystem interface. Or you can focus on accessing
various hardware and implementing drivers.

Note that in academic context it's called "OS research". The main difference is
that they're paid for it.

There are also parts of OSDEV you can skip over, like implementing a libc.
Most people will probably not implement a compiler or language, and just use
gcc and C (or if you have bad taste, clang and C++).

This repository probably focuses on OS concepts, or just wasting time.

It's never finished and it's pointless
--------------------------------------

An OS is a very large project. As a single person, it'll never be finished,
unless you're Linus or a mentally ill person (see TempleOS). The main point of
it is learning, and smugly demonstrating to others that you're able to master
the low level things.

So this is a learning experience (some use the pejorative "education"), not to
produce something useful. What it _could_ produce by exploring concepts are good
ideas (some use the pejorative "research").

Kernel
------

Most people will be sane and use the traditional kernel space and userspace
separation. You could be crazy and make a single address space OS. For the
latter, there is TempleOS for mentally ill grade, or something like Java OSes
or Microsoft Singularity for sane-but-dumb. (The latter mostly focus on
achieving privilege separation on the language level, so it might be
interesting if you want to create your own compiler. But a bit of a big project
to attack both compilers and OSes at once.)

For normal native code kernel/userspace OSes, you'll essentially start by
creating a primitive not-"kernel", that prints "hello world" in privileged
mode. Then you can work yourself forward by for example implementing kernel
memory management, and eventually a userspace process. At this point it'll be
obvious (but still hard) how to progress. The question will come up what kind
of system you're trying to create. The recommendation is creating a UNIX/POSIX
kernel, because that is the simplest and most proven OS design.

Libc
----

You'll probably need basic tools like printf() and memcpy(). My recommendation
is to either write them yourself (a bit of work and a stumbling block every time
you hit an unimplemented function), or to use a simple libc like musl.

musl is pretty simple, yet very complete and correct. They aim for full
C11/POSIX support. Unfortunately, the code is a bit too compact (they tend to
sacrifice compressed source code for readability), but it's better than what
you'll find in other libcs. Other libcs are either all sorts of broken and
buggy, or huge (and broken/buggy) like glibc. In fact musl also had plenty of
brokenness/bugs, but at least it's manageable due to its size. Also, musl does
not provide a bare metal mode, nor does it support OSes other than Linux. But
most likely it'll be faster to adjust musl, than porting some other libc to
your "OS" (which _will_ be a joke at such an early point anyway).

Personally I consider a libc a completely different task. If you include the
libc in OSDEV, you may as well include the compiler.

As musl shows, writing a libc yourself is possible, but a _big_ task. On the
other hand, you probably won't ever port real world software to your system,
so a partial and potentially buggy libc implementation will be perfectly fine.

Reminder to myself
------------------

I used an ad-hoc way to "redirect" musl syscalls to Linux. The musl devs are
open to better ways to retarget musl, but I don't expect much of it in the short
term. So I copied musl from the riscv-musl repository (commit 2fedc58ed98a8)
(since RISC-V isn't upstreamed yet in musl), used a musl _installation_, to
avoid some of the weird musl build script (some headers are generated by
scripts), and edited the RISC-V syscall scripts. Syscalls now call functions in
musl_emu.c. My approach breaks cancellation support (which the musl devs vividly
and repeatedly pointed out), but on the other hand cancellation is just an icky
UNIX thing that is rarely useful in the real world (due to some severe downsides
of cancellation), and which a hobby OS will never support anyway.

Build environment, linker scripts
---------------------------------

The kernel environment is similar to userspace programs, but some details are
pretty different. For example, you need to pass the correct compiler flags for
kernel development, or "bad" things could happen. This can be pretty confusing
over all. It's recommended to do what Linux does.

To get started, the most important thing is probably to interact with the
boot loader correctly. This means using a correct linker script, to make sure
the kernel is placed at the right place in memory. (I myself simply started out
with the Linux linker script for RISC-V, because linker scripts are an obscure
thing and usually full of crap.)

If your bootloader doesn't support ELF, it is recommended to use objcopy with
binary output to "flatten" it. You need to make sure that the addresses in
the ELF file matches with the addresses your sections (in fact, program headers)
are linked to.

If you want to use virtual memory, it may be necessary to relocate your kernel,
which is an extra messy part.

When you hit function calls to libgcc, look what the Linux kernel does.

What platform for hobby OS development
======================================

Obviously, an OS kernel is highly hardware dependent. A kernel may have large
parts of code that are as portable as userspace code, but even booting requires
platform-specific code. A real OS kernel like will have platform specific code
for each architecture. Of course everyone wants to be portable to everything,
but in practice you'll want to focus on one platform to get _something_ done.
Portability is hard.

It seems in OSDEV, x86 has been the preferred target architecture most of the
time,  although ARM is rising in popularity. Unfortunately, both of these
architectures are SHIT. You'll get held back by their pointless complexity and
legacy nonsense. I'm recommending RISC-V instead (see below).

x86
---

x86 is based on a tiny CPU from the 70ies. It wasn't really revolutionary or
even interesting back then, and as far as the ISA goes, it was never state of
the art in anything. It simply grew "organically", an euphemism that is used
when you only add to something without ever cleaning it up or properly designing
anything. IBM simply picked it because it was low cost or something, and then it
grew bigger along with the PC. The PC hardware isn't any better. Go search about
what the A20 gate is and why you need(ed?) to care about it, and you will
understand.

Getting started on x86 is not easy at all. You have to deal with at least 3 CPU
modes (16/32/64 bits), have to setup the weird protected mode descriptors, which
are particularly confusing because most of protected mode isn't used by modern
OSes (plus not fully implemented in 64 bit mode), and the CPU mechanisms itself
are kind of annoying (like the CPU pushing stuff on the stack when entering
exception handlers, and AFAIR _different_ stuff depending on type of exception).
Having done it myself, I really hated building CPU descriptors. Intel provides
the most performant CPUs you can buy, but as a platform, it's just not nice.

ARM
---

ARM on the other hand is a RISC architecture (i.e. followed state of the art
technology at some point) that was actually designed. Although it appears
somewhat elegant as an ISA, it approaches x86 in badness. In particular, there
are _many_ ways to build an ARM system, so you basically have to pick a specific
board. But granted, it's probably better than x86 for OSDEV. You probably can
easily find a board that supports the ISA variant you want, or qemu's virt
platform might make you happy.

RISC-V
------

RISC-V is a relatively new architecture. It's so new that it's hard to get real
hardware (not many full featured 64 bit CPUs on the market, forget about PCs).
It's basically an academic project that is now being commercialized. And maybe
due to the academic roots, the instruction set is extremely simple. While
you need to go through hundreds of pages of vendor docs to understand x86 and
ARM from basic instruction set to kernel mode operation, you can learn all of
RISC-V in an afternoon or so. Also, while the x86 and ARM docs are full of
dealing with legacy problems, the RISC-V docs are full of enlightening remarks
about ISA design and reasons for specific design choices.

RISC-V is supposed to scale from embedded to super computers, but it clearly
focuses on embedded and special-purpose applications rather than "PC" hardware,
maybe expecting even an even worse fragmented ecosystem than ARM. They expect
and _encourage_ that vendor add extensions to the ISA for special applications.
ARM does the opposite (you need a virtually unaffordable "architectural" license
to make changes to the ISA), and x86 is not available as configurable IP at all.

But they try to mitigate this by standardizing the CPU environment. There are
standard mechanisms for all the messy low level things. Some things are
abstracted by a well-defined firmware interface (that isn't a horrific
abomination like EFI, and can even be reimplemented without much trouble).

By now, toolchains and emulators are widely available and reasonably stable.
qemu provides full emulation, and gcc/binutils just work.

Thus I highly recommend RISC-V for hobby OS development. Maybe RISC-V makes it
almost too easy, and you miss out on the debugging "fun" x86 or ARM can provide.

My only gripe was with the qemu bootprocess, and the fact that you need to
provide the RISC-V firmware (OpenSBI) explicitly on the qemu CLI. Also, the
privileged mode docs are sort of unfinished (and _not_ finalized). Some things
are too vague. But it's nothing compared to what you'd have to go through on
x86.

The project in this repository uses RISC-V and requires RV64 + Sv48 MMU. This
does not exist in silicon anywhere AFAIK, so qemu with the virt platform is used
(see qemu-run.sh).

Should you make a microkernel or monolithic kernel?
===================================================

Anyone making attempts at OSdev has to deal with a million of choices they could
make. One of them is microkernel vs. monolithic kernel. Although it doesn't
matter: take this repository for example; this thing is so primitive and shitty
that the type of kernel can't even be distinguished at this stage. (I mean there
isn't even a single driver, for fuck's sake.)

Clearly the answer is that you should make a "monolithic" kernel. Just try to
make a small UNIX. Why not a microkernel? They're complex, involve tons of
design decisions (basically you need to design the entire system before deciding
on the kernel API, or it won't be very "micro"), and it will take a long time
until you get something that appears to be doing anything. Consider that real
microkernel systems don't even exist. Why fall for a hype from the 80ies that
turned out to be one of the largest failures of computer science research?
(Well, I'm not a historian, but a multiple decade effort resulting in almost
nothing seems pretty bad.)

Are Microkernels Stupid?
------------------------

Yes, they are. Both academic and commercial research made intensive efforts to
develop usable microkernels and microkernel based OSes. Absolutely nothing came
of it. Today, microkernels only have some niche uses, and OSes based on true
microkernels don't seem to exist. The height of microkernel failure was IBM's
"Workplace OS", that was a giant money grave and was eventually canceled. In
summary, microkernels didn't keep their promises.

The microkernel promise
-----------------------

Consider UNIX, which is basically the base for all other advanced OSes (WinNT
is just a UNIX buried under a lot of bloat): it has a kernel, that provides
services like hardware interactions, filesystems, process management,
networking, etc., and utilities in userspace (i.e. outside of the kernel) that
give the user access to these services. All these basic services run in the
kernel, which (from a userspace developer point of view) is a gigantic process
with a single address space. If this "process" crashes, you get a bluescreen.

For example, if someone finds a security issue in an obscure filesystem, an EVIL
HACKER could prepare a USB stick with a malicious filesystem on it, that
triggers the bug. Since there is no privilege separation between the filesystem
and the kernel, the EVIL HACKER could freely access the rest of the kernel (and
since the kernel is privileged, all of userspace), and basically do anything
with your system.

A microkernel OS would put each filesystem into a separate process. Then the bug
would, in theory, have no impact on your security at all.

Microkernels have about 2 fundamental principles: modularization, and privilege
separation of modules. They're sort of interconnected: obviously you can't do
privilege separation without some level of modularization. The "micro" here lies
in the fact that the kernel should provide the modularization and privilege
separation mechanism for "server" processes/components _only_, and all the rest
is done in userspace.

Researchers derived tons of promises from this. For example, microkernel based
OSes were claimed to be more robust and safer than monolithic OSes, because
one services are isolated, and crashes wouldn't take down the entire system (see
filesystem example above).

There were some more promises, but this isn't a damn academic paper.

Also note that microkernels did have some success in niches. But microkernel
_OSes_ did not.

Failure
-------

As mentioned above, nothing ever came of microkernels, and some of the attempts
to use them for desktop OSes ended in spectacular failures (like IBM's
Workplace OS). The main issues in general were complexity and bad performance.

I guess performance in particular killed the appeal of microkernels. I think
they arrived at the result that microkernel systems are twice as slow as
traditional system (from my hazy memory).

The reason is that crossing the privilege barrier is expensive. Since each
service is implemented in its own process, they need to use IPC to communicate
with each other. IPC needs to enter the kernel, switch address spaces, and
return control to a different process. This is expensive because it enters slow
paths in the CPU (this also makes syscalls expensive), flushes tons of caches,
and compared to function calls, requires packing/unpacking arguments to an IPC
transfer buffer.

The cost of crossing was high back then, and it's even higher now. With the
Spectre "vulnerability" (which I think is hugely inflated compared to other
security issues), it's guaranteed that crossing of privilege barrier will
remain expensive, as CPU designers will make sure to flush EVERYTHING when
crossing.

With monolithic OSes like Linux, crossing is expensive enough that Linux added
"userspace" syscalls, with a mechanism called vDSO. For example, you can query
the clock without actually entering the kernel. That they went through this
effort shows that the performance penalty from crossing is generally bad.
Microkernels have to pay even more, because IPC involves address space switches,
and the equivalent of a single syscall on a monolithic OS may require multiple
IPCs (depending on design).

In addition, a monolithic kernel can freely access the calling processes'
address space. On a microkernel OS, a server process is "not supposed" to be
able to access the address space of a calling process (for effective privilege
separation), so you need additional (expensive and complex) mechanisms to deal
with that.

Imagine you're implementing UNIX pipe() on a microkernel. The microkernel is not
supposed to even know about UNIX pipes or FDs; you need to implement this in a
server process. A write() call would need to be implemented as IPC call to
that server, and the data to write would be copied with the IPC, stored in a
temporary buffer. When another process calls read(), another IPC needs to be
performed, including copying all the data again. In a typical case of a blocked
reader, a monolithic kernel could do this with 1 instead of 2 switches, and 1
instead of 2 copies. (There are some more gross details and implementation
strategies to be considered. It's just an example.)

Many monolithic OSes allow implementing some types of drivers/services in
userspace, but they're typically slower and sometimes "stunted", and can't do
everything a kernel driver can do (consider FUSE on Linux, or writing "drivers"
with libusb).

Another issue is complexity. Modularization is hard, especially if you need to
achieve performance and security. Consider the pipe example above (or anything
else including networking and normal filesystems) - there are tons of complex
things you could try to optimize them, all which add complexity. How do you
protect OS services against DoS? How do you protect other userspace OSes from
misbehaving servers (did you ever deal with a frozen FUSE sshfs on Linux)?

I find it important that they really failed to create a fully modularized
system with _useful_ privilege separation, and instead they usually end up with
some central servers (that are single point of failures both in terms of
robustness and security). Sometimes they admit this, like in this relatively
recent academic work about MINIX (and which I'll conveniently not cite), where
the author admitted that some of the robustness claims are sort of bullshit.

A true microkernel also implements drivers as userspace processes. Lots of
hardware doesn't even allow implementing "unprivileged" drivers, because the
hardware can access the entire system memory anyway, or there are inherent
problems like needing to deassert level-triggered interrupts on shared interrupt
lines.

One of the wildest claims of the microkernel hype was that a single system could
provide multiple OS "personalities". So you could run OS/2 (hey it was relevant
back then) and Unix on the same microkernel, simply because you could implement
them as userspace components. IBM actually tried this (and guess what, they
failed). If you look at projects like wine or WinNT Linux emulation, you'll
realize that you don't really need such a kernel to achieve your goal, as well
as the fact that implementing an entire OS on top of another one is a
"difficult" task.

In summary, anything is probably always going to be simpler and more efficient
in monolithic kernels. It's even possible that "logical" security issues (that
go beyond buffer overflows in server code) are easier to avoid in monolithic
kernels.

Microkernels 2nd generation (German engineering)
------------------------------------------------

In the 90ies, a researcher (Jochen Liedtke) recognized that one of the major
problems of microkernels was IPC performance, and tried to find out whether
IPC really had to be slow. He determined that IPC is inherently slow, but that
the Mach implementation (and others) was _much_ slower than necessary. He
created L4, a microkernel optimized for raw IPC. The original L4 (L3) paper is
kind of an amazing read, because it's so focused on its goal and has good
results to show for it. It made quite an impact, and apparently all kernels in
this style are now called 2nd generation microkernels.

This didn't really answer how to build microkernel OSes, but it did prove that
Mach (US/Californian technology) is crap. This sort of confuses me, because
researchers of all kinds have tried to fix Mach performance for at least a
decade. You'd think they'd have tried the same as Liedtke.

Hybrid kernels (aka even more microkernel failure)
--------------------------------------------------

Sometimes there is talk about "hybrid" kernels, that include the best of both
worlds. But it's really just marketing they used when they wanted a microkernel,
but had a monolithic kernel.

(On the other hand, maybe this is the right label for attempts to design
operating systems that try to implement UNIX in a privilege separated manner,
more of that below. Some early microkernels also might have been called hybrid
kernels, e.g. due to having drivers in the kernel.)

For example, WinNT is occasionally called a hybrid kernel, but it's really not.
It's a traditional, albeit modular kernel. At some point WinNT even implemented
font rendering in the kernel (JESUS FUCKING CHRIST). WinNT was created in the
early 90ies, where the microkernel hype was at its height, but it was also clear
that a working OS wouldn't be a microkernel.

Apple OSX (aka macOS and many other confusing spellings) has parts based on Mach
(well it still uses Mach), but it's really a monolithic kernel that's mostly
FreeBSD. I'm not sure, but it's probably implemented as "co-located FreeBSD
personality", which is bullshit speak for "awful disgusting chimera of FreeBSD
and Mach all in the kernel". It seems Mach IPC on that system is mostly used as
equivalent for D-Bus on Linux (or actually the other way around), but I don't
know enough about it to really tell.

As far as I know, Apple uses co-located servers. Normally, Mach servers run in
their own process (and own address space, for privilege separation). Co-located
servers are Mach servers that are located in the kernel's address-space, but
otherwise behave like true Mach servers. Things that use IPC on Mach typically
use MIG (Mach Interface Generator) to generate IPC stub code from IDL (or so).
That means a function generated by MIG looks like a normal function, but
actually performs a Mach IPC call (it hides tricky parts like packing and
unpacking arguments to an IPC transfer buffer). Co-located servers are built
with MIG turning certain IPC calls into plain procedure calls (or well, at least
skipping the IPC syscall), which makes them much faster. The claim in the paper
that introduced it was that microkernels were all about modularization all along
(duh!), and privilege separation is not important for central servers like the
UNIX personality (duh!).

In summary, Apple OSX is a monolithic kernel built upon a microkernel as base.
Note that Apple tried porting Linux to Mach before that (why did they even try
this), but failed: http://mklinux.org/graphics/dpenguin.gif

Google Fuchsia is a new OS that is called "microkernel" by some. As is typical
of Google, the only information available is in their repository, although they
do provide some design docs. There are no papers, no plans, no outside
involvement. It's just a source dump, and some journalists circle jerking around
it get their information from those source dumps. It might be a 1st generation
microkernel or some variant of a hybrid kernel. It's interesting that it's
based on an open source project (LK), and that its author was hired by Google
(whether for/to work on Fuchsia or not, I don't know). Based on general
situation of Google tech, it's probably either crap, or even if not, Google
won't care one bit about anyone but themselves.

(Offtopic: Why does Google open source stuff but not care about open source?)
-----------------------------------------------------------------------------

Fuchsia (see above) really drives home how Google does internal development and
how it relates to open source.

Even though Google is hailed as major open source company (or some such), it is
toxic to the open source ecosystem. There are 2 things they do:

    1. Participating in development of open source software they do not "own"
    2. Provide as much source code of their software as they can in public,
       repositories (including history, often accepting 3rd party patches),

There is nothing wrong with that, except the way how they do this.

First off, why do they use open source? Because it's an enormous quantity of
relatively high quality software that is available free of charge, usually even
with (essentially) waived copyright. People will maintain it for free, and even
fix bugs and implement features for them for free. (Just imagine it. One of the
biggest and richest companies on the world, and you do stuff for them for...
free?)

The first problem is that Google devs won't fail to use their "leverage" of
the company they work for to force in their changes. For example, they might
have a patch that is not ready or even against the community, but with their
name and influence, they will get the change in anyway. Why would a project
allow this? Because Google devs have enough energy to do it. The company is
behind them (there's little doubt the dev's manager is usually convinced he's
doing the right thing, would be an asshole of a manager otherwise). They can
just spend a lot of time to "convince" the other devs that it's really
absolutely necessary. Further, Google may hire more project members to tilt
the development generation into their favor.

Last but not least, they could "hard fork" the entire project, and marginalize
the original project. In particular, this would remove project members that had
been hired by Google before, and would potentially remove infrastructure
provided by Google. Depending on the open source project and Google's
"involvement", this could range from disadvantageous to fatal for it.

Most of the time it won't go this far of course - because everyone knows that
going against a big company (that became very involved with the project) is
probably a bad idea or will lead to "drama". And probably that it will mean to
lose their support and contributions. You don't just bite the hand that feeds
you. Google can exert influence without anything "bad" happening.

This dynamic is not necessarily something individual Google devs use on purpose
or are even aware of, but it's definitely something Google as a company as a
whole participates in. In summary, their involvement with open source is
characterized by a certain mostly subtle ruthlessness/inconsideration, that
doesn't put them into a too good light.

Small projects are often just "grabbed" and essentially hard-forked, without
anything ever being contributed back. They probably do it if it's convenient.
(Look into the third_party directory of any Google project. You could probably
research how much of that software has non-upstreamed patches.)

The second thing from above, providing source code of the projects they develop
internally. This affects software like Android and Chrome, and of course
Fuchsia. The truth is, these are not real open source projects. There is no
community. There is (apparently) not even access to whatever place Google devs
use to discuss development (just their git logs and maybe gerrit).

Essentially, these repositories are only source dumps. It's a bit puzzling why
Google would even bother to make them public, without trying to create a
community around it (which is required for companies to get anything out of
open source developed by themselves). You could come up with a number of reasons
to do so: Google makes public as much as they can so they can tighten security
about the really important bits, or to appear "cool" to young people, or to
curb copyright violation claims by willingly exposing internal development to
the public ("hide in plain sight", if you want to be mean), or maybe they really
expect drive by contributions under these circumstances. It could also be to
make outside developers to become interested in the technology they're
developing (interested developers may teach themselves about the technology
before they're hired, which is obviously very good for the company).

Some people even like Google's source dumps, because it's better than nothing.
If some Google library or API is undocumented, they may be able to look into
the source code to find out how to use them. This also helps Google without
having to invest additional effort.

I think it's really just that Google _knows_ that publishing their source has
only advantages. The part about being cool is a real thing; just look at
Microsoft's behavior currently. Developers developers developers.

But the main takeaway here is that Google doesn't give a shit about anyone but
themselves. Their open source is not participation and building software in a
community - it's just for getting forward faster, and ultimately for making
money faster. This is why I claim that their open source is "toxic". Don't you
dare to claim they do it out of altruism or some open source ideology thing.

And the main consequence is that you should be careful about using Google
software. They won't care about your use case. They won't care about your
contributions (at least not if they seem to go against Google's desired
direction of development). They won't care about your tools. One of the dumbest
Google software thing is that you need to use one of their shitty build systems.
They will also use their shitty build system to build outside dependencies. It's
really shitty. Also have you ever used depot_tools? Be relieved if you haven't.
Google is essentially an ecosystem in itself, that's why it doesn't truly fit
into the open source ecosystem.

Look at others trying to use Google software. In almost all cases it's pure
chaos. Google will build stuff for themselves, it doesn't matter whether others
can use it. This starts at a pretty early and obvious point: the build system
and deployment. They won't care whether their own changes break other people's
software or use cases. Using Google software is a high maintenance matter. Also,
Google maybe have a certain degree of engineer elitism, but there's still plenty
of room for Google engineers to produce pure crap.

Note that this isn't a black and white thing. Google does contribute useful
things to community projects, open source Google software can be useful, and
nobody wants to hurt you anyway. It also depends on the individual Google dev
and how he/she/it works with the community. But if you don't see a tendency,
then whatever.

I also don't claim to be a Google expert, this is just my experience and what
I heard from others, plus a good deal of approximation. It's not Google specific
either. Most big companies offend in similar ways, just that I find Google's
behavior the most obnoxious.

(Strangely, Google is also closing projects sometimes. Apparently more and more
Android OS parts are getting closed (certain "apps", etc.). Likely this is to
prevent minor changes to the software by effectively Google competitors. Some
company could simply create an advantage over Google or partners by adding an
additional minor feature to a builtin app. This is the phone market, after all.)

(Also, did you ever notice how Google is getting rid of GPL software? Why would
they dislike GPL if they open source everything they own anyway? The only reason
I can come up with is because they don't essentially own it, like they do with
BSD licensed stuff. Maybe additional "duties" attached to the license makes
their management nervous, and they used GPL projects out of pure necessity.)

Microkernels today
------------------

I think there were some research OSes, which are all dead by now. GNU Hurd is
apparently a microkernel, but I don't know if it qualifies as real microkernel,
or if it has drivers in kernel space. Also, it's dead.

There is L4Linux, which is normal Linux ported to run on a L4 kernel. I'm not
really sure why they did this. For one, it was probably useful to evaluate IPC
performance in a real world setting in a meaningful way. They could simply
compare how much slower it ran than "native" Linux. Another thing is that you
get a full Linux system, with the possibility to run certain programs "outside"
of Linux in segregated L4 tasks, for the sake of realtime behavior and security.
This is probably great for certain embedded applications. If you really want to
know, their website has tons of academic papers to sift through.

Unsurprisingly, many uses of microkernels are in the embedded world, and
apparently mostly as some sort of secure hypervisor, with real OSes beneath
them. These are not "real" microkernel OSes, because of the limited scope of
their use cases. I suspect the rise of DRM will create actual reasons for
companies to care about "security", and adding a secure microkernel
("hypervisor" style) is the easiest way to do it.

Some projects, especially research or hobby OSes, also claim to be microkernels.
That's because microkernels are still cool, and at least wrt. research,
improving security (and thus privilege separation) is the only thing left to do.

As mentioned above, Fuchsia is a commercial OS project that is said to use a
microkernel.

Don't confuse microkernels with realtime OSes (RTOS). RTOSes are usually made to
be tiny, but often they lack the general purpose direction or privilege
separation mechanisms that are essential to microkernels. For most embedded
uses, microkernels are actually too heavy, and the services they do provide are
kind of useless.

What happened instead of microkernels
-------------------------------------

I think virtualization (virtual machines, containers, hypervisors, etc.)
essentially replaced microkernels. You want to isolate a process in some way
because it could be dangerous? Just run an entire OS in a VM! You want to
sandbox parts of your server? Just use containers! (Well, that's questionable,
but it's a typical reason given for using such setups.)

Apparently there was also progress in general software development that managed
to tame monolithic kernels: they're no longer crashy pieces of shit (most time).
It's pretty rare that Windows or Linux crashes. If you ask your grandpa, he'll
tell you that Win 9x used to crash at least once every day.

Nowadays, you can write userspace drivers in some cases, e.g. FUSE and libusb,
which fulfills another microkernel promise.

Linux keeps pushing for very weird stuff, like extending BPF (originally byte
code to customize network packet filtering in the kernel) to be more or less
general purpose sandboxed userspace code running in the kernel. I eagerly await
the day when Linux allows implementing entire filesystems or drivers in BPF,
just so I can laugh at the world. BPF seems to be mainly abused to circumvent
the expensive syscall barrier, though.

What should OS research focus on?
---------------------------------

If you ask me (fortunately nobody does), trying to introduce privilege
separation. When I talked to Rich Felker (the most UNIX person I've ever
encountered), there does seem to be room for this, and the belief that UNIX
can be efficiently implemented on a, uh, tiny kernel. (Specifically, he
challenged my claim that it has to be inefficient - though I'm still skeptic.)
It all comes down to finer grained privilege separation, not microkernels as
general purpose kernel.

Microkernels have been traditionally general purpose. The idea is that they
should be neutral (policy free), so that arbitrary OS mechanisms can be
implemented on top of it. Once the microkernel is designed and implemented, its
development is finished, and the rest is up to the OS implemented on top of it.
The L4 inventor's most central claim was that microkernels must be small to be
efficient (and probably to guarantee other properties). Policy-freedom was
somehow critically connected to it (go read his paper if you're interested).

Is it possible that a kernel should really be designed to implement a subset of
POSIX, or specifically allow implementing POSIX with low overhead? Such a kernel
would still be relatively small (though probably much larger than a L4 style
kernel), but would violate the "policy free" property of microkernels. Thus, it
wouldn't be a microkernel. You'd have to use another label.

Why UNIX? UNIX is the most widespread OS API, and it does most things in pretty
straight-forward ways. Everything will have files and file handles. Everything
needs to implement read() and write(). Take Windows: it's just a very complex
take on UNIX, with some parts destroying the fundamental elegance of UNIX. (Like
making many things not behave like file handles - ever used Windows sockets,
the console API, or named pipes? If you know the UNIX equivalents you'll
probably agree with me, even if you dislike POSIX.) My claim is that UNIX is
the fundamental general purpose OS interface (if you go beyond extremely
embedded use cases), and there is no need for microkernels to be truly general.

The real questions are:

    1. How could such a system be designed?
    2. Could such a kernel be more efficient than a POSIX OS implemented on top
       of a true microkernel?
    3. Could such a kernel be as efficient as Linux or BSD?

The project in this repository is aiming to explore how to implement POSIX in a
way that leaves most icky POSIX stuff outside of the kernel, which in turn was
provoked by not liking some parts of POSIX, but recognizing the absolute
pointlessness of coming up with new interfaces. So so answering these questions
is not really what I'm aiming for, although it might be exploring 1. Also, the
reference required in 2 does not even exist.

Microkernel research projects partially try to answer the question of "could
a kernel built on top of a microkernel be as efficient as Linux" (so not really
1 to 3 from  above), see projects like L4Linux. But note that they do not try
to build _OSes_ on top of microkernels; L4Linux for example still has a kernel
process that can freely access the entire RAM mapped for L4Linux (including all
Linux userspace processes).
